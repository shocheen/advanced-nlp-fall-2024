---
title: Transfer Learning in NLP
---

### Content
Pretraining: Architectures and Methods

### Slides: 

[Transformers / Pretraining / Finetuning]({{ site.url }}/{{ site.baseurl }}/assets/slides/lecture2.pdf)

### Reading Material 

- Attention is all you need (2017) [[link](https://arxiv.org/abs/1706.03762)]

- BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding  [[link](https://arxiv.org/abs/1810.04805)]

Optional readings:

- The Illustrated Transformer [[link](https://jalammar.github.io/illustrated-transformer/)]

- The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [[link](https://jalammar.github.io/illustrated-bert/)]

- T5 [[link](https://arxiv.org/abs/1910.10683)]
- The Illustrated GPT2 [[link]()]

- RoBERTa, A Robustly Optimized BERT Pretraining Approach  [[link](https://arxiv.org/abs/1907.11692)]

