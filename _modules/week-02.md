---
title: Introduction - Course Overview (August 26)
---

### Content
Pretraining: Architectures and Methods

### Slides: 

Transformers

Masked Language Models

### Reading Material 

- Attention is all you need (2017) [[link](#)]

- BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding  [[link](#)]

Optional readings:

- ELMo, Deep Contextualized Representations  [[link](#)]

- ULMFit, Universal Language Model Fine-tuning for Text Classification  [[link](#)]

RoBERTa, A Robustly Optimized BERT Pretraining Approach  [[link](#)]

